"""
Coreference Resolution using spaCy + neuralcoref + Context Tracking
"""

import os
import logging
import spacy
from typing import List, Dict, Any, Optional

from .conversation_context import ConversationContext

logger = logging.getLogger(__name__)

class CoreferenceResolver:
    """
    Resolves coreferences (pronouns and references) in text using NLP
    Uses ConversationContext for semantic understanding and state tracking
    """
    
    def __init__(self):
        self.nlp = None
        self.use_neuralcoref = os.getenv('USE_NEURALCOREF', 'true').lower() == 'true'
        self.context = None  # Will be initialized after nlp model loads
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize spaCy model with coreferee (modern coreference resolution)"""
        try:
            model_name = os.getenv('SPACY_MODEL', 'en_core_web_sm')
            logger.info(f"üîÑ Loading spaCy model: {model_name}")
            
            self.nlp = spacy.load(model_name)
            
            if self.use_neuralcoref:
                # Try coreferee first (modern, maintained alternative)
                try:
                    import coreferee
                    self.nlp.add_pipe('coreferee')
                    logger.info("‚úÖ coreferee added to pipeline (modern coreference resolution)")
                    self.coref_method = 'coreferee'
                except ImportError:
                    # Fallback to neuralcoref if available
                    try:
                        import neuralcoref
                        neuralcoref.add_to_pipe(self.nlp)
                        logger.info("‚úÖ neuralcoref added to pipeline")
                        self.coref_method = 'neuralcoref'
                    except ImportError:
                        logger.warning("‚ö†Ô∏è No neural coreference library available, using rule-based resolution")
                        self.use_neuralcoref = False
                        self.coref_method = 'rule_based'
            else:
                self.coref_method = 'rule_based'
            
            # Initialize conversation context tracker
            self.context = ConversationContext(self.nlp)
            
            logger.info("‚úÖ Coreference resolver initialized")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize model: {e}")
            raise
    
    def _rebuild_context_from_history(self, conversation_history: List[Dict[str, str]]) -> None:
        """
        Rebuild conversation context from history
        This allows the context tracker to understand the full conversation state
        """
        # Reset context
        self.context = ConversationContext(self.nlp)
        
        # Process each user message to build up context
        for i, msg in enumerate(conversation_history):
            if msg.get('role') == 'user':
                self.context.update_from_user_message(msg['content'], i)
        
        # Log current context state
        if self.context.current_topic:
            logger.debug(f"üìä Context state: {self.context.get_context_summary()}")
    
    def resolve(
        self,
        message: str,
        conversation_history: List[Dict[str, str]],
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Resolve coreferences in a message using conversation history
        
        Args:
            message: The current message to resolve
            conversation_history: List of previous messages
            options: Additional options for resolution
        
        Returns:
            Dict with originalMessage, resolvedMessage, replacements, method
        """
        options = options or {}
        
        # Extract screen content from options (for screen_intelligence intents)
        screen_content = options.get('screenContent')
        intent_type = options.get('intentType')
        
        if screen_content:
            logger.info(f"üñ•Ô∏è  Screen content provided for {intent_type} intent")
            logger.debug(f"   Screen content preview: '{screen_content[:150]}...'")
        
        try:
            # Rebuild conversation context from history
            # This allows the context tracker to understand the full conversation state
            self._rebuild_context_from_history(conversation_history)
            
            # Build full context document and track message boundaries
            context_parts = []
            message_roles = []  # Track whether each part is from user or assistant
            for msg in conversation_history[-5:]:  # Last 5 messages for context
                context_parts.append(msg['content'])
                message_roles.append(msg.get('role', 'user'))  # Default to user if role not specified
            context_parts.append(message)
            message_roles.append('user')  # Current message is always from user
            
            full_text = " ".join(context_parts)
            
            # Store message boundaries and roles for later use
            self._message_boundaries = []
            current_pos = 0
            for i, part in enumerate(context_parts):
                start_pos = current_pos
                end_pos = start_pos + len(part)
                self._message_boundaries.append({
                    'start': start_pos,
                    'end': end_pos,
                    'role': message_roles[i],
                    'text': part
                })
                current_pos = end_pos + 1  # +1 for the space separator
            
            # Process with spaCy
            doc = self.nlp(full_text)
            
            # Get resolved text based on available method
            if self.coref_method == 'coreferee':
                # coreferee analyzes full context but we only resolve current message
                resolved_message = self._resolve_with_coreferee(doc, message, full_text)
                method = "coreferee"
                
                # If coreferee didn't find anything, try simple pronoun fallback
                if resolved_message == message:
                    logger.debug("‚ö†Ô∏è Coreferee found no chains - trying simple pronoun fallback")
                    resolved_message = self._simple_pronoun_fallback(message, conversation_history, doc, screen_content)
                    if resolved_message != message:
                        method = "simple_fallback"
                        logger.info(f"‚úÖ Simple fallback resolved: '{message}' ‚Üí '{resolved_message}'")
                    else:
                        method = "none"
                    
            elif self.coref_method == 'neuralcoref' and hasattr(doc._, 'coref_resolved'):
                resolved_full = doc._.coref_resolved
                method = "neuralcoref"
                # Extract just the current message (last part)
                resolved_message = self._extract_current_message(
                    resolved_full,
                    message,
                    len(context_parts) - 1
                )
            else:
                # No coreference method available - return original message
                # LLM has conversation history and can handle context itself
                logger.debug("‚ö†Ô∏è No coreference method available - returning original message unchanged")
                resolved_message = message
                method = "none"
            
            # Find replacements
            replacements = self._find_replacements(message, resolved_message)
            
            # Determine if this query needs conversation context
            # True if: coreferences were found OR query has elliptical reference patterns
            needs_context = self._check_needs_context(message, replacements, method)
            
            return {
                "originalMessage": message,
                "resolvedMessage": resolved_message,
                "replacements": replacements,
                "method": method,
                "needsContext": needs_context
            }
            
        except Exception as e:
            logger.error(f"‚ùå Resolution error: {e}", exc_info=True)
            # Return original message if resolution fails
            # Still check if context is needed even on error
            needs_context = self._check_needs_context(message, [], "fallback")
            return {
                "originalMessage": message,
                "resolvedMessage": message,
                "replacements": [],
                "method": "fallback",
                "needsContext": needs_context
            }
    
    def _resolve_with_coreferee(self, doc, message: str, full_text: str) -> str:
        """
        Resolve coreferences using coreferee chains
        Analyzes full conversation context but only replaces pronouns in current message
        """
        resolved = message
        replacements_made = []
        
        # Find where the current message starts in the full text
        message_start_pos = full_text.rfind(message)
        
        # coreferee provides coreference chains via doc._.coref_chains
        if hasattr(doc._, 'coref_chains') and doc._.coref_chains:
            logger.debug(f"Found {len(doc._.coref_chains)} coreference chain(s)")
            
            for chain in doc._.coref_chains:
                try:
                    # Chain is a list of Mention objects
                    # Find the main reference - prefer MOST RECENT non-pronoun mention before current message
                    main_mention_text = None
                    main_mention_pos = -1
                    current_message_pronouns = []
                    non_pronoun_candidates = []  # Track all candidates for ambiguity detection
                    
                    # Sort mentions by position (earliest first for processing)
                    sorted_mentions = sorted(chain, key=lambda m: m.token_indexes[0])
                    
                    for mention in sorted_mentions:
                        # Get the mention's token span
                        mention_tokens = [doc[i] for i in mention.token_indexes]
                        mention_text = ' '.join([t.text for t in mention_tokens])
                        
                        # Get position of first token in mention
                        first_token = mention_tokens[0]
                        token_pos = first_token.idx
                        
                        # Check if this mention is in the current message
                        in_current_message = token_pos >= message_start_pos
                        
                        # Check if this is a pronoun mention
                        is_pronoun = mention_tokens[0].text.lower() in ['he', 'she', 'it', 'they', 'him', 'her', 'his', 'their', 'them', 'its']
                        
                        if in_current_message and is_pronoun:
                            # Pronoun in current message - mark for replacement
                            current_message_pronouns.append(mention_tokens[0].text)
                        elif in_current_message and not is_pronoun:
                            # CRITICAL: Entity in current message (same sentence as pronoun)
                            # This should have HIGHEST priority - use position 10000000 to ensure it wins
                            if any(t.ent_type_ in ['PERSON', 'ORG', 'PRODUCT', 'GPE'] for t in mention_tokens):
                                entity_tokens = [t for t in mention_tokens if t.ent_type_ != '']
                                if entity_tokens:
                                    first_ent_token = entity_tokens[0]
                                    ent_start = first_ent_token.i
                                    ent_end = first_ent_token.i + 1
                                    
                                    while ent_start > 0 and doc[ent_start - 1].ent_type_ == first_ent_token.ent_type_:
                                        ent_start -= 1
                                    while ent_end < len(doc) and doc[ent_end].ent_type_ == first_ent_token.ent_type_:
                                        ent_end += 1
                                    
                                    candidate_text = ' '.join([doc[i].text for i in range(ent_start, ent_end)])
                                else:
                                    candidate_text = mention_text
                                
                                # Use very high position to prioritize current message entities
                                non_pronoun_candidates.append((candidate_text, 10000000, True))
                                main_mention_text = candidate_text
                                main_mention_pos = 10000000
                                logger.info(f"üéØ Prioritizing entity from current message: '{candidate_text}'")
                            elif any(t.pos_ == 'PROPN' for t in mention_tokens):
                                non_pronoun_candidates.append((mention_text, 10000000, True))
                                main_mention_text = mention_text
                                main_mention_pos = 10000000
                                logger.info(f"üéØ Prioritizing proper noun from current message: '{mention_text}'")
                        elif not in_current_message and not is_pronoun:
                            # Non-pronoun mention from earlier in conversation
                            # CRITICAL: Prioritize entities from USER messages over ASSISTANT messages
                            # This prevents picking entities the AI mentioned over entities the user explicitly referenced
                            
                            # Determine if this mention is from a user or assistant message
                            is_from_user = False
                            if hasattr(self, '_message_boundaries'):
                                for boundary in self._message_boundaries:
                                    if boundary['start'] <= token_pos < boundary['end']:
                                        is_from_user = (boundary['role'] == 'user')
                                        break
                            
                            # Prefer PERSON/ORG entities
                            if any(t.ent_type_ in ['PERSON', 'ORG', 'PRODUCT', 'GPE'] for t in mention_tokens):
                                # Get the FULL entity text by finding the complete entity span
                                # This ensures we get "Isaac Herzog" not just "Herzog"
                                entity_tokens = [t for t in mention_tokens if t.ent_type_ != '']
                                if entity_tokens:
                                    # Get the full entity by finding all tokens in the same entity
                                    first_ent_token = entity_tokens[0]
                                    # Find all consecutive tokens with the same entity type
                                    ent_start = first_ent_token.i
                                    ent_end = first_ent_token.i + 1
                                    
                                    # Expand backwards to get full entity
                                    while ent_start > 0 and doc[ent_start - 1].ent_type_ == first_ent_token.ent_type_:
                                        ent_start -= 1
                                    
                                    # Expand forwards to get full entity
                                    while ent_end < len(doc) and doc[ent_end].ent_type_ == first_ent_token.ent_type_:
                                        ent_end += 1
                                    
                                    # Extract full entity text
                                    candidate_text = ' '.join([doc[i].text for i in range(ent_start, ent_end)])
                                else:
                                    candidate_text = mention_text
                                
                                # Prioritize user messages: add 1000000 to position if from user
                                # This ensures user entities always win over assistant entities
                                priority_pos = token_pos + (1000000 if is_from_user else 0)
                                non_pronoun_candidates.append((candidate_text, priority_pos, is_from_user))
                                
                                # Update if this has higher priority
                                if priority_pos > main_mention_pos:
                                    main_mention_text = candidate_text
                                    main_mention_pos = priority_pos
                                    logger.debug(f"üìå Candidate: '{candidate_text}' (pos: {token_pos}, user: {is_from_user}, priority: {priority_pos})")
                            elif any(t.pos_ == 'PROPN' for t in mention_tokens):
                                # Proper noun phrase - use the full mention text
                                priority_pos = token_pos + (1000000 if is_from_user else 0)
                                non_pronoun_candidates.append((mention_text, priority_pos, is_from_user))
                                
                                # Update if this has higher priority
                                if priority_pos > main_mention_pos:
                                    main_mention_text = mention_text
                                    main_mention_pos = priority_pos
                                    logger.debug(f"üìå Candidate: '{mention_text}' (pos: {token_pos}, user: {is_from_user}, priority: {priority_pos})")
                    
                    # Safety check: Only resolve if we have clear reference
                    # UPDATED: Instead of skipping ambiguous chains, use the HIGHEST PRIORITY mention
                    # Priority: current message (10000000) > user messages (1000000+pos) > assistant messages (pos)
                    unique_candidates = set(text for text, *_ in non_pronoun_candidates)
                    
                    if len(unique_candidates) > 1:
                        # Multiple entities in chain - use the highest priority one
                        # This is already stored in main_mention_text due to our priority-based update logic
                        logger.debug(f"‚ö†Ô∏è Multiple entities in chain: {unique_candidates}")
                        logger.info(f"‚úÖ Using highest priority mention: '{main_mention_text}' (priority: {main_mention_pos})")
                    
                    # Replace pronouns in current message with main reference
                    if main_mention_text and current_message_pronouns:
                        for pronoun in current_message_pronouns:
                            resolved = resolved.replace(pronoun, main_mention_text, 1)
                            replacements_made.append(f"'{pronoun}' ‚Üí '{main_mention_text}'")
                            logger.info(f"‚úÖ Resolved '{pronoun}' ‚Üí '{main_mention_text}'")
                
                except Exception as e:
                    logger.warning(f"Error processing coreference chain: {e}")
                    logger.exception(e)
                    continue
            
            if replacements_made:
                logger.info(f"üéØ Coreferee resolved: {', '.join(replacements_made)}")
        else:
            logger.debug("No coreference chains found by coreferee")
        
        return resolved
    
    def _rule_based_resolution(
        self,
        message: str,
        conversation_history: List[Dict[str, str]]
    ) -> str:
        """
        ‚ö†Ô∏è DEPRECATED - This method is no longer used as fallback
        
        Rule-based coreference resolution has ~30% success rate and 70% risk of mangling queries.
        Better to return original message and let LLM handle context from conversation history.
        
        Kept for reference/testing purposes only.
        
        Simple rule-based coreference resolution
        ONLY resolves actual coreferences (common nouns referring to entities)
        Does NOT resolve proper nouns like "the russia", "the USA", etc.
        """
        resolved = message
        
        # Pattern: "the [noun]" or "that [noun]"
        # Only match lowercase common nouns (not proper nouns)
        import re
        reference_pattern = r'\b(the|that|this)\s+([a-z][a-z\s]+?)\b'
        
        matches = list(re.finditer(reference_pattern, message, re.IGNORECASE))
        
        # Known coreference nouns (common nouns that can refer to entities)
        coreference_nouns = {
            'president', 'person', 'man', 'woman', 'guy', 'leader', 'minister',
            'company', 'organization', 'business',
            'movie', 'show', 'book', 'song', 'film', 'series',
            'product', 'device', 'app',
            'city', 'country', 'place', 'location',
            'current president', 'prime minister', 'current leader'
        }
        
        for match in matches:
            reference = match.group(0)
            noun = match.group(2).strip().lower()
            
            # CRITICAL: Only resolve if this is a known coreference noun
            # Skip proper nouns like "the russia", "the england", etc.
            if noun not in coreference_nouns:
                logger.debug(f"‚è≠Ô∏è Skipping '{reference}' - not a coreference noun")
                continue
            
            # Find antecedent in conversation history
            antecedent = self._find_antecedent(noun, conversation_history)
            
            if antecedent:
                resolved = resolved.replace(reference, antecedent, 1)
                logger.debug(f"‚úÖ Resolved '{reference}' ‚Üí '{antecedent}'")
        
        # Handle pronouns: he, she, it, they
        # BUT skip expletive/dummy pronouns (e.g., "it takes", "it is", "it seems")
        pronoun_pattern = r'\b(he|she|it|they|him|her|his|their)\b'
        pronoun_matches = list(re.finditer(pronoun_pattern, message, re.IGNORECASE))
        
        # Expletive patterns where "it" is NOT a reference
        expletive_patterns = [
            r'\bit\s+(is|was|will\s+be|would\s+be|has\s+been|had\s+been)',  # it is, it was, etc.
            r'\bit\s+(takes|took|will\s+take)',  # it takes
            r'\bit\s+(seems|appears|looks)',  # it seems
            r'\bit\s+(depends|matters)',  # it depends
            r'\bhow\s+(long|far|much)\s+does\s+it',  # how long does it
            r'\bwhat\s+does\s+it',  # what does it
        ]
        
        for match in pronoun_matches:
            pronoun = match.group(0).lower()
            
            # Skip if this is an expletive pronoun
            if pronoun == 'it':
                is_expletive = False
                for pattern in expletive_patterns:
                    if re.search(pattern, message, re.IGNORECASE):
                        is_expletive = True
                        break
                
                if is_expletive:
                    continue  # Skip this pronoun
            
            antecedent = self._find_pronoun_antecedent(conversation_history)
            
            if antecedent:
                resolved = resolved.replace(match.group(0), antecedent, 1)
        
        return resolved
    
    def _find_antecedent(
        self,
        noun: str,
        conversation_history: List[Dict[str, str]]
    ) -> Optional[str]:
        """
        Find the antecedent (what the noun refers to) in conversation history
        Uses semantic matching to find entities that match the noun type
        """
        import re
        
        # Map common nouns to entity types
        noun_to_entity_type = {
            'president': 'PERSON',
            'person': 'PERSON',
            'man': 'PERSON',
            'woman': 'PERSON',
            'guy': 'PERSON',
            'company': 'ORG',
            'organization': 'ORG',
            'movie': 'WORK_OF_ART',
            'show': 'WORK_OF_ART',
            'book': 'WORK_OF_ART',
            'song': 'WORK_OF_ART',
            'product': 'PRODUCT',
            'city': 'GPE',
            'country': 'GPE',
            'place': 'GPE'
        }
        
        target_entity_type = noun_to_entity_type.get(noun.lower())
        
        # Look for matching entities in recent messages
        for msg in reversed(conversation_history[-5:]):
            try:
                doc = self.nlp(msg['content'])
                
                # If we know the entity type, look for that specific type
                if target_entity_type:
                    for ent in doc.ents:
                        if ent.label_ == target_entity_type:
                            logger.debug(f"Found {target_entity_type} antecedent for '{noun}': {ent.text}")
                            return ent.text
                
                # Fallback: return first named entity
                if doc.ents:
                    logger.debug(f"Found generic antecedent for '{noun}': {doc.ents[0].text}")
                    return doc.ents[0].text
                    
            except Exception as e:
                logger.warning(f"Error finding antecedent: {e}")
                continue
        
        return None
    
    def _find_pronoun_antecedent(
        self,
        conversation_history: List[Dict[str, str]]
    ) -> Optional[str]:
        """
        Find the antecedent for pronouns (he, she, it, they)
        Uses spaCy NER to intelligently find people, organizations, products, etc.
        Returns None if ambiguous (multiple different entities found)
        """
        import re
        
        # Collect all entities from recent messages
        all_entities_by_type = {
            'PERSON': [],
            'ORG': [],
            'PRODUCT': [],
            'GPE': [],
            'WORK_OF_ART': [],
            'EVENT': []
        }
        
        # Look for named entities in recent messages using spaCy NER
        for msg in reversed(conversation_history[-3:]):
            try:
                # Use spaCy to extract named entities
                doc = self.nlp(msg['content'])
                
                for ent in doc.ents:
                    if ent.label_ in all_entities_by_type:
                        all_entities_by_type[ent.label_].append(ent.text)
                    
            except Exception as e:
                logger.warning(f"Error extracting entities: {e}")
                continue
        
        # Check for ambiguity - if multiple different entities of same type, it's ambiguous
        for entity_type in ['PERSON', 'ORG', 'PRODUCT', 'GPE', 'WORK_OF_ART', 'EVENT']:
            entities = all_entities_by_type[entity_type]
            if entities:
                unique_entities = set(entities)
                if len(unique_entities) > 1:
                    logger.debug(f"‚ö†Ô∏è Ambiguous: Multiple {entity_type} entities found: {unique_entities}")
                    return None  # Ambiguous - don't guess
                else:
                    # Only one unique entity of this type - safe to use
                    logger.debug(f"‚úÖ Found unambiguous {entity_type} antecedent: {entities[0]}")
                    return entities[0]
        
        # Fallback: Look for proper nouns in most recent message only
        if conversation_history:
            try:
                most_recent = conversation_history[-1]['content']
                
                # Fallback: Look for capitalized multi-word phrases (likely proper nouns)
                # But exclude common words like "President", "United States", etc.
                proper_noun_pattern = r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)\b'
                matches = re.findall(proper_noun_pattern, most_recent)
                
                # Filter out common non-entity phrases
                excluded_phrases = ['United States', 'President', 'According To', 'The User']
                filtered_matches = [m for m in matches if m not in excluded_phrases]
                
                if filtered_matches:
                    logger.debug(f"Found proper noun antecedent: {filtered_matches[0]}")
                    return filtered_matches[0]
                    
            except Exception as e:
                logger.warning(f"Error processing message for antecedent: {e}")
                pass  # Fall through to return None
        
        return None
    
    def _simple_pronoun_fallback(
        self,
        message: str,
        conversation_history: List[Dict[str, str]],
        doc,
        screen_content: Optional[str] = None
    ) -> str:
        """
        Context-aware fallback using ConversationContext for semantic resolution
        
        Handles:
        1. Elliptical patterns (e.g., "second best", "what about X")
        2. Pronouns (it, them, they, this, that)
        3. Short answers to AI questions
        
        Uses ConversationContext for semantic understanding instead of brittle regex patterns
        """
        import re
        
        # Get last assistant message if exists
        last_assistant_message = None
        if conversation_history:
            for msg in reversed(conversation_history):
                if msg.get('role') == 'assistant':
                    last_assistant_message = msg.get('content', '')
                    break
        
        # Try context-aware resolution methods in order of specificity
        
        # 1. Try elliptical resolution (e.g., "what's the second best", "what about X")
        resolved = self.context.resolve_elliptical(message)
        if resolved:
            return resolved
        
        # 2. Try pronoun resolution (e.g., "tell me about it")
        resolved = self.context.resolve_pronoun(message, doc)
        if resolved:
            return resolved
        
        # 3. Try short answer resolution (e.g., "France" after "Which region?")
        if last_assistant_message:
            resolved = self.context.resolve_short_answer(message, last_assistant_message)
            if resolved:
                return resolved
        
        # 4. No resolution found - return original message
        logger.debug("‚è≠Ô∏è No context-aware resolution found, returning original message")
        return message
        
        # FAST GATE: Skip if no pronouns/demonstratives AND no elliptical patterns
        has_pronouns = any(word in message.lower() for word in [
            ' it ', ' them ', ' they ', ' this ', ' that ',
            ' he ', ' she ', ' his ', ' her ', ' him ', ' their '
        ])
        
        # SPECIAL CASE: Short answer to AI question
        # If the message is very short (1-3 words) and the previous message is from the assistant
        # and contains a question, treat this as an answer that needs context
        word_count = len(message.split())
        if not has_pronouns and not has_elliptical and word_count <= 3:
            # Check if previous message is from assistant and contains a question
            if conversation_history:
                last_msg = conversation_history[-1]
                if last_msg.get('role') == 'assistant':
                    last_content = last_msg.get('content', '')
                    # Check if it's a question (contains ?, or contains question phrases)
                    question_phrases = ['could you', 'can you', 'would you', 'please specify', 'which', 'what', 'where', 'when', 'who', 'how']
                    is_question = '?' in last_content or any(
                        q in last_content.lower() for q in question_phrases
                    )
                    
                    if is_question:
                        logger.info(f"üéØ Detected short answer to AI question: '{message}'")
                        # Try to resolve by extracting context from the question
                        return self._resolve_answer_to_question(message, conversation_history)
        
        if not has_pronouns and not has_elliptical:
            logger.info("‚è≠Ô∏è No pronouns or elliptical patterns detected, skipping coreference resolution")
            return message
        
        # If we have elliptical patterns but no pronouns, try to resolve the subject
        # IMPORTANT: Pass only the conversation history WITHOUT the current message
        if has_elliptical and not has_pronouns:
            logger.info(f"üéØ Detected elliptical pattern without pronouns: '{message}'")
            # conversation_history here is the ORIGINAL history passed to resolve(), 
            # which does NOT include the current message yet
            return self._resolve_elliptical_reference(message, conversation_history)
        
        # CRITICAL: Special handling for demonstrative pronouns with nouns (e.g., "that folder", "that person", "that place")
        # Use spaCy to detect ANY demonstrative + noun combination, not just a hardcoded list
        # This is safe because demonstrative + noun is always referential, never a relative pronoun
        doc = self.nlp(message)
        demonstrative_noun_phrases = []
        
        for i, token in enumerate(doc):
            # Check if token is a demonstrative (that/this)
            if token.lower_ in ['that', 'this'] and token.tag_ == 'DT':
                # Check if next token is a noun
                if i + 1 < len(doc):
                    next_token = doc[i + 1]
                    if next_token.pos_ in ['NOUN', 'PROPN']:
                        # Found a demonstrative + noun phrase
                        phrase = f"{token.text} {next_token.text}"
                        demonstrative_noun_phrases.append({
                            'phrase': phrase,
                            'demonstrative': token.text,
                            'noun': next_token.text,
                            'start': token.idx,
                            'end': next_token.idx + len(next_token.text)
                        })
                        logger.info(f"üéØ Detected demonstrative noun phrase: '{phrase}' (noun: {next_token.text}, POS: {next_token.pos_})")
        
        # If we found demonstrative + noun phrases, try to resolve them
        if demonstrative_noun_phrases and conversation_history:
            # Look at the most recent assistant response
            last_message = conversation_history[-1]
            if last_message.get('role') == 'assistant':
                last_content = last_message.get('content', '')
                logger.info(f"üîç Looking for referents in assistant response: {last_content[:200]}...")
                
                # FLEXIBLE APPROACH: Use spaCy to extract ALL meaningful noun phrases from the response
                # This works for ANY scenario: folders, people, places, documents, etc.
                assistant_doc = self.nlp(last_content)
                all_candidates = []
                
                # Extract named entities (PERSON, ORG, GPE, LOC, PRODUCT, etc.)
                for ent in assistant_doc.ents:
                    if ent.label_ in ['PERSON', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'FAC', 'WORK_OF_ART', 'EVENT']:
                        all_candidates.append(ent.text)
                        logger.info(f"‚úÖ Found entity: '{ent.text}' ({ent.label_})")
                
                # Extract noun chunks (noun phrases like "bills folder", "the restaurant", etc.)
                for chunk in assistant_doc.noun_chunks:
                    # Only include if it's a meaningful noun phrase (not just pronouns or articles)
                    if chunk.root.pos_ in ['NOUN', 'PROPN']:
                        # Clean up: remove leading articles/determiners
                        cleaned = chunk.text
                        if cleaned.lower().startswith(('the ', 'a ', 'an ')):
                            cleaned = ' '.join(cleaned.split()[1:])
                        
                        if cleaned and len(cleaned) > 1:
                            all_candidates.append(cleaned)
                            logger.info(f"‚úÖ Found noun chunk: '{cleaned}'")
                
                # Remove duplicates while preserving order
                seen = set()
                unique_candidates = []
                for candidate in all_candidates:
                    candidate_lower = candidate.lower()
                    if candidate_lower not in seen:
                        seen.add(candidate_lower)
                        unique_candidates.append(candidate)
                
                all_candidates = unique_candidates
                logger.info(f"üìä Total unique candidates: {len(all_candidates)}")
                
                if all_candidates:
                    # FLEXIBLE SELECTION: Use spaCy entity labels to match demonstrative noun with candidates
                    # E.g., "that person" ‚Üí prefer PERSON entities
                    # E.g., "that folder" ‚Üí prefer candidates that look like paths or were extracted as file/folder entities
                    first_phrase = demonstrative_noun_phrases[0]
                    noun_type = first_phrase['noun'].lower()
                    
                    # Score each candidate based on how well it matches the noun type
                    scored_candidates = []
                    
                    for candidate in all_candidates:
                        score = 0
                        candidate_doc = self.nlp(candidate)
                        
                        # Check if candidate has entity labels that match the noun semantically
                        for ent in candidate_doc.ents:
                            # PERSON entities for person-related nouns
                            if ent.label_ == 'PERSON' and any(t.pos_ == 'NOUN' and t.lower_ in self.nlp(noun_type)[0].lower_ for t in candidate_doc):
                                score += 10
                            # GPE/LOC/FAC for place-related nouns
                            elif ent.label_ in ['GPE', 'LOC', 'FAC']:
                                score += 5
                            # ORG for organization-related nouns
                            elif ent.label_ == 'ORG':
                                score += 5
                        
                        # Boost score for paths when noun is file/folder related
                        if '/' in candidate:
                            score += 8
                        
                        # Boost score for capitalized multi-word phrases (likely proper nouns)
                        if candidate[0].isupper() and ' ' in candidate:
                            score += 3
                        
                        scored_candidates.append((candidate, score))
                        logger.debug(f"üìä Candidate '{candidate}' scored {score}")
                    
                    # Sort by score (highest first) and pick the best match
                    scored_candidates.sort(key=lambda x: x[1], reverse=True)
                    referent = scored_candidates[0][0]
                    logger.info(f"üéØ Selected best match: '{referent}' (score: {scored_candidates[0][1]})")
                    
                    # CRITICAL: Append the referent AFTER the noun, don't replace the phrase
                    # E.g., "that folder" ‚Üí "that folder bills"
                    # E.g., "that person" ‚Üí "that person Bob"
                    # E.g., "this place" ‚Üí "this place Paris"
                    first_phrase = demonstrative_noun_phrases[0]
                    
                    # Insert the referent after the demonstrative phrase
                    resolved = message[:first_phrase['end']] + f' {referent}' + message[first_phrase['end']:]
                    logger.info(f"‚úÖ Resolved demonstrative: '{message}' ‚Üí '{resolved}'")
                    return resolved
                else:
                    logger.warning(f"‚ö†Ô∏è No candidates found in assistant response")
        
        if not pronouns_to_resolve:
            return message
        
        # CRITICAL: Special case for "this" - check if it appears right after AI response
        if 'this' in pronouns_to_resolve and conversation_history:
            last_message = conversation_history[-1]
            # If last message was from assistant and current message mentions "this"
            if last_message.get('role') == 'assistant':
                # Check if this is a request to transform/translate the AI's response
                transform_patterns = [
                    r'\b(give me|translate|convert|make|turn|put|write|show me)\s.*\bthis\b',
                    r'\bthis\s+in\s+(chinese|spanish|french|english|japanese)',
                    r'\b(summarize|explain|rewrite)\s.*\bthis\b'
                ]
                
                is_transform_request = any(re.search(pattern, message, re.IGNORECASE) for pattern in transform_patterns)
                
                if is_transform_request:
                    logger.info(f"üéØ Detected transform request for AI response: '{message}'")
                    # Return a special marker that the calling code can recognize
                    # This tells the system that "this" refers to the previous AI response
                    resolved = re.sub(r'\bthis\b', 'the previous response', message, flags=re.IGNORECASE)
                    logger.info(f"üîÑ Resolved 'this' to refer to previous AI response: '{resolved}'")
                    return resolved
        
        # CRITICAL: Process current message first to identify nouns we should EXCLUDE
        current_msg_doc = self.nlp(message)
        current_msg_nouns = set()
        current_msg_entities = []  # Track entities from current message
        
        for token in current_msg_doc:
            if token.pos_ in ['NOUN', 'PROPN']:
                current_msg_nouns.add(token.text.lower())
        
        # CRITICAL FIX: Check if pronoun can be resolved within the current message first
        # E.g., "do I have a bills folder where is it" ‚Üí "it" refers to "bills folder" in same sentence
        # BUT: Skip this if screen_content is provided - screen entities should take priority
        if not screen_content:
            for ent in current_msg_doc.ents:
                if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT', 'WORK_OF_ART', 'FAC', 'LOC', 'EVENT']:
                    current_msg_entities.append(ent.text)
            
            # Also extract noun phrases from current message
            for chunk in current_msg_doc.noun_chunks:
                # Only include if it's not just a pronoun
                if chunk.root.pos_ in ['NOUN', 'PROPN']:
                    current_msg_entities.append(chunk.text)
            
            # If we found entities in the current message, prioritize them
            if current_msg_entities:
                logger.info(f"üéØ Found {len(current_msg_entities)} potential referents in current message: {current_msg_entities}")
        else:
            logger.info(f"üñ•Ô∏è  Screen content provided - skipping current message entity extraction")
        
        # Collect named entity candidates from conversation history
        named_entity_candidates = []
        noun_candidates = []
        # Track which candidates came from highlighted text (don't re-process these)
        highlighted_text_candidates = set()
        
        # UI/Button text patterns to exclude (these are not meaningful referents)
        ui_button_patterns = [
            'log in', 'sign up', 'log out', 'sign in', 'click here', 'learn more',
            'get started', 'try free', 'start trial', 'buy now', 'add to cart',
            'search', 'submit', 'cancel', 'ok', 'yes', 'no', 'close', 'open'
        ]
        
        # CRITICAL: Screen-aware resolution - extract entities from screen content FIRST
        # These have HIGHEST priority for screen_intelligence intents
        if screen_content:
            logger.info(f"üñ•Ô∏è  Extracting entities from screen content for screen-aware resolution")
            logger.info(f"   Screen content preview: '{screen_content[:200]}...'")
            
            screen_doc = self.nlp(screen_content)
            
            # Extract entities from screen content
            for ent in screen_doc.ents:
                if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT', 'WORK_OF_ART', 'FAC', 'LOC', 'EVENT']:
                    named_entity_candidates.append(ent.text)
                    logger.info(f"‚úÖ Found screen entity: '{ent.text}' ({ent.label_})")
            
            # Extract noun chunks (method names, function names, etc.)
            for chunk in screen_doc.noun_chunks:
                if chunk.root.pos_ in ['NOUN', 'PROPN']:
                    # Clean up: remove leading articles/determiners
                    cleaned = chunk.text
                    if cleaned.lower().startswith(('the ', 'a ', 'an ')):
                        cleaned = ' '.join(cleaned.split()[1:])
                    
                    if cleaned and len(cleaned) > 1:
                        named_entity_candidates.append(cleaned)
                        logger.info(f"‚úÖ Found screen noun chunk: '{cleaned}'")
            
            # SPECIAL: Extract method/function names (e.g., "raw()", "repeat()", etc.)
            # These are common in code documentation and should be prioritized
            import re
            method_pattern = r'\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\(\)'
            methods = re.findall(method_pattern, screen_content)
            for method in methods:
                method_with_parens = f"{method}()"
                named_entity_candidates.append(method_with_parens)
                logger.info(f"‚úÖ Found screen method: '{method_with_parens}'")
            
            logger.info(f"üìä Extracted {len(named_entity_candidates)} candidates from screen content")
            
            # If we found candidates from screen, prioritize them and skip conversation history
            if named_entity_candidates:
                logger.info(f"üéØ Using screen entities as primary candidates (skipping conversation history)")
                # Skip conversation history processing - we have screen entities
            else:
                logger.info(f"‚ÑπÔ∏è No entities found in screen content, falling back to conversation history.")
        
        # Only process conversation history if no screen content was provided or no entities were found in it
        if not screen_content or not named_entity_candidates:
            for msg in reversed(conversation_history[-3:]):  # Last 3 messages
                content = msg['content']
                
                # CRITICAL FIX: If this is a wrapped highlighted text message, extract the actual content
                # E.g., "The highlighted text is: Arfrix Dela Cruz" ‚Üí "Arfrix Dela Cruz"
                # E.g., "The highlighted text is: I love Chick Filet" ‚Üí "Chick Filet"
                if content.startswith("The highlighted text is:"):
                    actual_content = content.split(":", 1)[1].strip()
                    logger.info(f"üìé Extracted highlighted content: '{actual_content}' from wrapped message")
                    
                    # Process the highlighted content with spaCy to extract entities
                    highlighted_doc = self.nlp(actual_content)
                    
                    # PRIORITY 1: Extract all consecutive proper nouns (PROPN) - most reliable for names
                    propn_sequence = []
                    for token in highlighted_doc:
                        if token.pos_ == 'PROPN':
                            propn_sequence.append(token.text)
                        elif propn_sequence:
                            # We hit a non-PROPN after collecting some PROPNs, stop here
                            break
                    
                    if propn_sequence:
                        entity = ' '.join(propn_sequence)
                        named_entity_candidates.append(entity)
                        highlighted_text_candidates.add(entity.lower())  # Mark as from highlighted text
                        logger.info(f"‚úÖ Extracted proper noun sequence from highlighted text: '{entity}'")
                        continue
                    
                    # PRIORITY 2: Try named entity recognition (fallback for when PROPN fails)
                    highlighted_entities = [ent.text for ent in highlighted_doc.ents 
                                           if ent.label_ in ['PERSON', 'ORG', 'PRODUCT', 'GPE', 'WORK_OF_ART', 'FAC', 'LOC']]
                    
                    if highlighted_entities:
                        # Use the first entity found
                        entity = highlighted_entities[0]
                        named_entity_candidates.append(entity)
                        logger.info(f"‚úÖ Extracted NER entity from highlighted text: '{entity}'")
                        continue
                    
                    # Last resort: If it's a short phrase (‚â§4 words) with no verbs, use it as-is
                    has_verb = any(token.pos_ == 'VERB' for token in highlighted_doc)
                    word_count = len(actual_content.split())
                    if not has_verb and word_count <= 4:
                        named_entity_candidates.append(actual_content)
                        logger.info(f"‚úÖ Using short highlighted text as entity: '{actual_content}'")
                        continue
                    
                    logger.warning(f"‚ö†Ô∏è Could not extract entity from highlighted text: '{actual_content}'")
                
                # Process with spaCy to find entities
                msg_doc = self.nlp(content)
            
                # First priority: Named entities (these are most likely referents)
                # CRITICAL: Merge adjacent entities that form compound references (e.g., "Genesis 6")
                entities = list(msg_doc.ents)
                
                # DEBUG: Log what spaCy detected
                logger.info(f"üîç spaCy detected {len(entities)} entities in: '{msg['content']}'")
                for ent in entities:
                    logger.info(f"   - Entity: '{ent.text}' (label: {ent.label_}, start: {ent.start}, end: {ent.end})")
                
                merged_entities = []
                i = 0
                while i < len(entities):
                    current_ent = entities[i]
                    
                    # Check if next entity is a number that should be merged (e.g., "Genesis" + "6")
                    if i + 1 < len(entities):
                        next_ent = entities[i + 1]
                        # Merge if: current is PROPN/WORK_OF_ART and next is CARDINAL/MONEY/ORDINAL
                        # AND they're adjacent (no words between them)
                        if (current_ent.label_ in ['WORK_OF_ART', 'PERSON', 'ORG', 'GPE'] or 
                            any(t.pos_ == 'PROPN' for t in current_ent)) and \
                           next_ent.label_ in ['CARDINAL', 'MONEY', 'ORDINAL', 'QUANTITY'] and \
                           next_ent.start == current_ent.end:
                            # Merge them
                            merged_text = f"{current_ent.text} {next_ent.text}"
                            logger.info(f"üîó Merging entities: '{current_ent.text}' + '{next_ent.text}' ‚Üí '{merged_text}'")
                            merged_entities.append((merged_text, 'MERGED'))
                            i += 2  # Skip both entities
                            continue
                    
                    merged_entities.append((current_ent.text, current_ent.label_))
                    i += 1
                
                for ent_text, ent_label in merged_entities:
                    # Include all major entity types that could be pronoun referents
                    if ent_label in ['PERSON', 'ORG', 'GPE', 'PRODUCT', 'WORK_OF_ART', 'LANGUAGE', 
                                     'NORP', 'FAC', 'LOC', 'EVENT', 'LAW', 'MERGED']:
                        # Exclude if this entity appears in current message (likely not a referent)
                        if ent_text.lower() not in current_msg_nouns:
                            # CRITICAL: Skip UI button text (e.g., "Log In/Sign Up")
                            is_ui_button = any(pattern in ent_text.lower() for pattern in ui_button_patterns)
                            if is_ui_button:
                                logger.debug(f"‚è≠Ô∏è Skipping UI button text: '{ent_text}'")
                                continue
                            
                            # CRITICAL FIX: Clean verbs from entity text
                            # If entity is "open Slack", extract only "Slack"
                            # BUT: Keep PERSON entities intact to preserve full names
                            if ent_label == 'PERSON':
                                # Keep full person names as-is (e.g., "Arfrix Dela Cruz")
                                named_entity_candidates.append(ent_text)
                                logger.debug(f"üë§ Keeping full PERSON entity: '{ent_text}'")
                            elif ent_label != 'MERGED':  # Don't re-parse merged entities
                                ent_doc = self.nlp(ent_text)
                                entity_tokens = [t for t in ent_doc if t.pos_ in ['NOUN', 'PROPN', 'ADJ', 'NUM']]
                                if entity_tokens:
                                    cleaned_entity = ' '.join([t.text for t in entity_tokens])
                                    # Double-check cleaned entity isn't UI text
                                    if not any(pattern in cleaned_entity.lower() for pattern in ui_button_patterns):
                                        named_entity_candidates.append(cleaned_entity)
                                else:
                                    # Fallback to original if no noun/propn/adj found
                                    if not is_ui_button:
                                        named_entity_candidates.append(ent_text)
                            else:
                                # Merged entities are already clean
                                named_entity_candidates.append(ent_text)
                
                # Second priority: Proper nouns (NNP) that aren't in current message
                for token in msg_doc:
                    if token.tag_ == 'NNP' and token.text.lower() not in current_msg_nouns:
                        # CRITICAL FIX: Only extract noun/proper noun tokens, NOT verbs
                        # This prevents capturing "open Slack" instead of just "Slack"
                        noun_phrase = ' '.join([t.text for t in token.subtree if t.pos_ in ['NOUN', 'PROPN', 'ADJ'] and t.dep_ not in ['aux', 'auxpass', 'cop']])
                        if noun_phrase and noun_phrase.lower() not in current_msg_nouns:
                            noun_candidates.append(noun_phrase)
        
        # CRITICAL FIX: Prioritize entities from current message over conversation history
        # This prevents picking up irrelevant entities like "ai mcp services" from file paths
        if current_msg_entities:
            # Use current message entities first
            all_candidates = current_msg_entities
            logger.info(f"‚úÖ Using {len(all_candidates)} candidates from current message (prioritized)")
        else:
            # Fall back to conversation history
            all_candidates = named_entity_candidates + noun_candidates
            logger.info(f"‚è≠Ô∏è No candidates in current message, using {len(all_candidates)} from conversation history")
        
        if all_candidates:
            # Remove duplicates while preserving order
            seen = set()
            unique_candidates = []
            for c in all_candidates:
                if c.lower() not in seen:
                    seen.add(c.lower())
                    unique_candidates.append(c)
            
            # CRITICAL: For screen-related questions, prioritize content entities over UI elements
            # E.g., "what chapter is this on" should refer to "Genesis 6" not "Log In/Sign Up"
            screen_question_keywords = ['chapter', 'page', 'section', 'article', 'reading', 'viewing', 'watching']
            is_screen_question = any(keyword in message.lower() for keyword in screen_question_keywords)
            
            if is_screen_question and len(unique_candidates) > 1:
                # Prioritize candidates that look like content (not UI buttons)
                content_candidates = []
                for candidate in unique_candidates:
                    # Check if this looks like content (has numbers, or is a proper noun phrase)
                    has_number = any(char.isdigit() for char in candidate)
                    is_multi_word = len(candidate.split()) > 1
                    is_ui_like = any(pattern in candidate.lower() for pattern in ui_button_patterns)
                    
                    if (has_number or is_multi_word) and not is_ui_like:
                        content_candidates.append(candidate)
                        logger.info(f"üìÑ Prioritizing content candidate: '{candidate}' for screen question")
                
                # Use content candidates if found, otherwise fall back to all candidates
                if content_candidates:
                    referent = content_candidates[0]
                    logger.info(f"üéØ Selected content referent: '{referent}' for screen question")
                else:
                    referent = unique_candidates[0]
            else:
                referent = unique_candidates[0]
            
            # CRITICAL FIX: Skip re-processing if this came from highlighted text
            # Highlighted text has already been properly extracted (e.g., "Arfrix Dela Cruz")
            if referent.lower() not in highlighted_text_candidates:
                # CRITICAL FIX: Strip verb prefixes from referent
                # If referent is "open Slack", we want just "Slack"
                # BUT: Preserve full entity names (PERSON, ORG, PRODUCT, GPE, etc.)
                referent_doc = self.nlp(referent)
                
                # Check if this contains a named entity - if so, extract ONLY the entity
                # Priority order: PERSON > ORG > PRODUCT > GPE > WORK_OF_ART > others
                entity_priority = ['PERSON', 'ORG', 'PRODUCT', 'GPE', 'WORK_OF_ART', 'FAC', 'LOC', 'EVENT', 'NORP']
                extracted_entity = None
                
                for entity_type in entity_priority:
                    entities = [ent for ent in referent_doc.ents if ent.label_ == entity_type]
                    if entities:
                        extracted_entity = entities[0].text
                        logger.info(f"üè∑Ô∏è Extracted {entity_type} entity from text: '{extracted_entity}'")
                        break
                
                if extracted_entity:
                    # E.g., "I love eating at Chick Filet" ‚Üí "Chick Filet"
                    # E.g., "doing very well Mark Shultz in life" ‚Üí "Mark Shultz"
                    referent = extracted_entity
                else:
                    # Common command verbs to strip
                    command_verbs = {'open', 'close', 'start', 'stop', 'launch', 'quit', 'kill', 'run', 'execute'}
                    
                    noun_tokens = []
                    for token in referent_doc:
                        # Skip if it's a verb OR if it's in our command verb list
                        if token.pos_ == 'VERB' or token.lemma_.lower() in command_verbs:
                            logger.info(f"üö´ Skipping verb/command: '{token.text}' (pos: {token.pos_}, lemma: {token.lemma_})")
                            continue
                        # Keep nouns, proper nouns, and adjectives
                        if token.pos_ in ['NOUN', 'PROPN', 'ADJ']:
                            noun_tokens.append(token.text)
                    
                    # If we found noun tokens, use them; otherwise keep original
                    if noun_tokens:
                        cleaned_referent = ' '.join(noun_tokens)
                        if cleaned_referent != referent:
                            logger.info(f"üßπ Cleaned referent: '{referent}' ‚Üí '{cleaned_referent}'")
                            referent = cleaned_referent
                    else:
                        logger.warning(f"‚ö†Ô∏è No noun tokens found in referent '{referent}', keeping original")
                    
                    # DEBUG: Log what tokens were extracted
                    logger.info(f"üîç Extracted noun tokens: {noun_tokens}")
            else:
                logger.info(f"‚úÖ Skipping re-processing for highlighted text candidate: '{referent}'")
            
            logger.info(f"üéØ Simple fallback found referent: '{referent}' for pronouns: {pronouns_to_resolve}")
            
            # Replace pronouns with referent
            resolved = message
            for pronoun in pronouns_to_resolve:
                # Case-insensitive replacement
                resolved = re.sub(
                    r'\b' + pronoun + r'\b',
                    referent,
                    resolved,
                    count=1,
                    flags=re.IGNORECASE
                )
            
            return resolved
        
        # No referent found
        logger.debug("‚ö†Ô∏è Simple fallback: No referent found")
        return message
    
    def _resolve_elliptical_reference(
        self,
        message: str,
        conversation_history: List[Dict[str, str]]
    ) -> str:
        """
        Resolve elliptical references like "second best", "another one", "the next"
        by extracting the subject from previous conversation context.
        
        E.g., "What's the best lasagna?" ‚Üí "What's the second best?" 
        Should resolve to: "What's the second best lasagna?"
        """
        import re
        
        if not conversation_history:
            logger.info("‚ö†Ô∏è No conversation history for elliptical resolution")
            return message
        
        # DEBUG: Log conversation history
        logger.debug(f"üìã Conversation history has {len(conversation_history)} messages")
        for i, msg in enumerate(conversation_history):
            logger.debug(f"   [{i}] {msg.get('role')}: {msg.get('content', '')[:50]}...")
        
        # Look at the most recent user message (skip assistant responses)
        # CRITICAL: Skip the first message if it matches the current message (it might be included)
        last_user_message = None
        for msg in reversed(conversation_history):
            if msg.get('role') == 'user':
                # Skip if this is the current message itself
                if msg.get('content', '').strip() != message.strip():
                    last_user_message = msg.get('content', '')
                    break
        
        if not last_user_message:
            logger.info("‚ö†Ô∏è No previous user message found for elliptical resolution")
            return message
        
        logger.info(f"üîç Analyzing previous user message: '{last_user_message}'")
        
        # SPECIAL CASE: "what about X" / "how about Y" - context-aware topic shift
        # BUT: Check if X itself is an elliptical pattern (e.g., "the second best")
        # vs a new concrete subject (e.g., "cheese cake")
        what_about_match = re.search(r'\b(what about|how about)\s+(.+?)(?:\?|\.\.\.|$)', message, re.IGNORECASE)
        if what_about_match:
            potential_subject = what_about_match.group(2).strip()
            
            # Check if the "subject" is actually an elliptical pattern itself
            # (e.g., "the second best", "the next one", "another option")
            elliptical_subject_patterns = [
                r'^(?:the\s+)?(?:first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\s+(?:best|worst|largest|smallest|biggest|most|least)$',
                r'^(?:the\s+)?(?:next|last|previous|another|other)\s+(?:one|option|choice)$',
            ]
            
            is_elliptical_subject = any(re.match(pattern, potential_subject, re.IGNORECASE) for pattern in elliptical_subject_patterns)
            
            if is_elliptical_subject:
                # This is NOT a topic shift - it's an elliptical reference
                # E.g., "what about the second best" should become "what's the second best [SUBJECT]"
                logger.info(f"üîÑ 'what about {potential_subject}' is an elliptical reference, not a topic shift")
                # Fall through to regular elliptical resolution below
            else:
                # This IS a topic shift - extract frame and apply to new subject
                # E.g., "what about cheese cake" ‚Üí "what's the best cheese cake"
                logger.info(f"üîÑ Detected topic shift pattern: 'what about {potential_subject}'")
                
                # Extract the query frame from previous message (best/worst/second best/etc.)
                # We want to extract just the ranking part (e.g., "the second best", "the best")
                # not the question word (what's/which)
                frame_pattern = r'(?:what\'?s?|which)\s+((?:the\s+)?(?:(?:first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\s+)?(?:best|worst|largest|smallest|biggest|most|least))'
                frame_match = re.search(frame_pattern, last_user_message, re.IGNORECASE)
                
                if frame_match:
                    # Extract just the ranking part (group 1), not the question word
                    frame = frame_match.group(1)
                    logger.info(f"‚úÖ Extracted query frame: '{frame}'")
                    
                    # Reconstruct query with new subject
                    resolved = f"what's {frame} {potential_subject}"
                    logger.info(f"‚úÖ Resolved topic shift: '{message}' ‚Üí '{resolved}'")
                    return resolved
                else:
                    logger.info("‚ö†Ô∏è Could not extract query frame from previous message, treating as new query")
                    return message
        
        # Extract the subject/topic from the previous message
        # Common patterns:
        # - "What's the best [SUBJECT]" ‚Üí extract SUBJECT
        # - "Tell me about [SUBJECT]" ‚Üí extract SUBJECT
        # - "Show me [SUBJECT]" ‚Üí extract SUBJECT
        
        subject_patterns = [
            r'(?:what\'?s?|which|who\'?s?)\s+(?:the\s+)?(?:best|worst|largest|smallest|biggest|most|least)\s+(.+?)(?:\s+in\s+|\s+for\s+|\s+of\s+|\?|\.\.\.|$)',
            r'(?:tell me about|show me|find|search for)\s+(.+?)(?:\?|\.\.\.|$)',
            r'(?:what|which)\s+(.+?)(?:\s+is\s+|\s+are\s+|\?|\.\.\.|$)',
        ]
        
        extracted_subject = None
        for pattern in subject_patterns:
            match = re.search(pattern, last_user_message, re.IGNORECASE)
            if match:
                extracted_subject = match.group(1).strip()
                logger.info(f"‚úÖ Extracted subject from pattern '{pattern}': '{extracted_subject}'")
                break
        
        # If no pattern matched, try spaCy to extract the main noun phrase
        if not extracted_subject:
            logger.info("üîç No pattern matched, using spaCy to extract subject")
            prev_doc = self.nlp(last_user_message)
            
            # Look for the root noun phrase (subject of the sentence)
            for chunk in prev_doc.noun_chunks:
                # Skip pronouns and articles
                if chunk.root.pos_ in ['NOUN', 'PROPN']:
                    # Clean up: remove leading articles
                    cleaned = chunk.text
                    if cleaned.lower().startswith(('the ', 'a ', 'an ')):
                        cleaned = ' '.join(cleaned.split()[1:])
                    
                    if cleaned and len(cleaned) > 1:
                        extracted_subject = cleaned
                        logger.info(f"‚úÖ Extracted subject from noun chunk: '{extracted_subject}'")
                        break
        
        if not extracted_subject:
            logger.warning("‚ö†Ô∏è Could not extract subject from previous message")
            return message
        
        # Now insert the subject into the current message
        # Patterns to handle:
        # - "what's the second best" ‚Üí "what's the second best [SUBJECT]"
        # - "show me another one" ‚Üí "show me another [SUBJECT]"
        # - "the next" ‚Üí "the next [SUBJECT]"
        
        insertion_patterns = [
            # Pattern: "what about the [ordinal] [superlative]" ‚Üí convert to "what's the [ordinal] [superlative] [SUBJECT]"
            (r'\b(?:what|how)\s+about\s+((?:the\s+)?(?:first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\s+(?:best|worst|largest|smallest|biggest|most|least))', r"what's \1 " + extracted_subject),
            # Pattern: "what's the [ordinal] [superlative]" ‚Üí insert subject after superlative
            (r'(what\'?s?\s+the\s+(?:first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\s+(?:best|worst|largest|smallest|biggest|most|least))', r'\1 ' + extracted_subject),
            # Pattern: "[ordinal] [superlative]" ‚Üí insert subject after superlative
            (r'((?:first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\s+(?:best|worst|largest|smallest|biggest|most|least))', r'\1 ' + extracted_subject),
            # Pattern: "another one" ‚Üí replace "one" with subject
            (r'\banother\s+one\b', 'another ' + extracted_subject),
            # Pattern: "the next" ‚Üí insert subject after "next"
            (r'\bthe\s+next\b', 'the next ' + extracted_subject),
        ]
        
        resolved = message
        for pattern, replacement in insertion_patterns:
            new_resolved = re.sub(pattern, replacement, resolved, flags=re.IGNORECASE)
            if new_resolved != resolved:
                logger.info(f"‚úÖ Resolved elliptical: '{resolved}' ‚Üí '{new_resolved}'")
                resolved = new_resolved
                break
        
        return resolved
    
    def _resolve_answer_to_question(
        self,
        message: str,
        conversation_history: List[Dict[str, str]]
    ) -> str:
        """
        Resolve short answers to AI questions by extracting context from the question.
        
        E.g., 
        AI: "Could you please specify a region"
        User: "France"
        Should resolve to: "second best cheese cake in France"
        
        The key is to find what the AI was asking about in the conversation history.
        """
        import re
        
        if not conversation_history:
            logger.info("‚ö†Ô∏è No conversation history for answer resolution")
            return message
        
        # Get the last assistant message (the question)
        last_assistant_msg = None
        for msg in reversed(conversation_history):
            if msg.get('role') == 'assistant':
                last_assistant_msg = msg.get('content', '')
                break
        
        if not last_assistant_msg:
            logger.info("‚ö†Ô∏è No previous assistant message found")
            return message
        
        logger.info(f"üîç Analyzing AI question: '{last_assistant_msg[:100]}...'")
        
        # Extract what the AI was asking about
        # Common patterns:
        # - "Could you please specify a [THING]" ‚Üí extract THING
        # - "Which [THING] would you like?" ‚Üí extract THING
        # - "What [THING] are you interested in?" ‚Üí extract THING
        
        question_patterns = [
            r'(?:specify|provide|tell me|give me)\s+(?:a|an|the)?\s*(\w+)',
            r'(?:which|what)\s+(\w+)',
        ]
        
        asked_about = None
        for pattern in question_patterns:
            match = re.search(pattern, last_assistant_msg, re.IGNORECASE)
            if match:
                asked_about = match.group(1).strip()
                logger.info(f"‚úÖ AI was asking about: '{asked_about}'")
                break
        
        # Now find the actual topic from earlier in the conversation
        # Look for user queries with ranking/comparison context (best, second best, etc.)
        # We want to find the most recent query that has explicit ranking keywords
        topic_context = None
        fallback_context = None
        
        for msg in reversed(conversation_history[:-1]):  # Skip the last assistant message
            if msg.get('role') == 'user':
                content = msg.get('content', '')
                # Check for explicit ranking keywords
                has_explicit_ranking = any(keyword in content.lower() for keyword in 
                    ['second best', 'third best', 'fourth best', 'fifth best',
                     'first best', 'best', 'worst', 'largest', 'smallest'])
                
                if has_explicit_ranking:
                    topic_context = content
                    logger.info(f"‚úÖ Found topic context with ranking: '{topic_context}'")
                    break
                elif not fallback_context and len(content.split()) > 2:
                    # Keep as fallback if no ranking context is found
                    fallback_context = content
        
        if not topic_context:
            topic_context = fallback_context
            if topic_context:
                logger.info(f"‚úÖ Found fallback topic context: '{topic_context}'")
        
        if not topic_context:
            logger.info("‚ö†Ô∏è Could not find topic context")
            return message
        
        # Extract the main subject/query from the topic context
        # E.g., "what's the second best cheese cake" ‚Üí extract "second best cheese cake"
        subject_patterns = [
            # Pattern with ordinal + superlative + subject (e.g., "second best cheese cake")
            r'(?:what\'?s?|which|who\'?s?)\s+(?:the\s+)?((?:(?:first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\s+)?(?:best|worst|largest|smallest|biggest|most|least)\s+[\w\s]+?)(?:\s+in\s+|\s+for\s+|\s+of\s+|\?|$)',
            # Pattern for "what about X" (e.g., "what about cheese cake")
            r'(?:what about|how about)\s+(.+?)(?:\?|$)',
        ]
        
        extracted_topic = None
        for pattern in subject_patterns:
            match = re.search(pattern, topic_context, re.IGNORECASE)
            if match:
                extracted_topic = match.group(1).strip()
                logger.info(f"‚úÖ Extracted topic: '{extracted_topic}'")
                break
        
        # If we couldn't extract a topic, the context itself might be elliptical (e.g., "What's the second best")
        # In this case, look further back for the actual subject
        if not extracted_topic:
            logger.info("‚ö†Ô∏è Topic context is elliptical, looking further back for subject")
            # Extract the ranking from the elliptical context
            ranking_match = re.search(r'((?:first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\s+)?(?:best|worst|largest|smallest|biggest|most|least)', topic_context, re.IGNORECASE)
            ranking = ranking_match.group(0) if ranking_match else None
            
            if ranking:
                logger.info(f"‚úÖ Extracted ranking from elliptical context: '{ranking}'")
                # Look for the most recent message with a concrete subject
                # Prioritize "what about X" messages as they indicate topic shifts
                for msg in reversed(conversation_history[:-1]):
                    if msg.get('role') == 'user':
                        content = msg.get('content', '')
                        
                        # First check for "what about X" pattern (topic shift)
                        what_about_match = re.search(r'(?:what about|how about)\s+([\w\s]+?)(?:\?|$)', content, re.IGNORECASE)
                        if what_about_match:
                            subject = what_about_match.group(1).strip()
                            extracted_topic = f"{ranking} {subject}"
                            logger.info(f"‚úÖ Combined ranking with 'what about' subject: '{extracted_topic}'")
                            break
                        
                        # Otherwise try to extract from ranking patterns
                        for pattern in subject_patterns:
                            match = re.search(pattern, content, re.IGNORECASE)
                            if match:
                                potential_topic = match.group(1).strip()
                                # Check if this has a concrete noun (not just "best" or "second best")
                                if len(potential_topic.split()) > 1:  # More than just "best"
                                    # Extract just the subject noun
                                    subject_match = re.search(r'(?:best|worst|largest|smallest|biggest|most|least)\s+([\w\s]+?)(?:\s+in\s+|\s+for\s+|\s+of\s+|\?|$)', potential_topic, re.IGNORECASE)
                                    if subject_match:
                                        subject = subject_match.group(1).strip()
                                        extracted_topic = f"{ranking} {subject}"
                                        logger.info(f"‚úÖ Combined elliptical ranking with subject: '{extracted_topic}'")
                                        break
                        if extracted_topic:
                            break
        
        if not extracted_topic:
            logger.info("‚ö†Ô∏è Could not extract topic from context")
            return message
        
        # Construct the resolved message
        # Format: "[topic] in/for [answer]"
        if asked_about and asked_about.lower() in ['region', 'location', 'place', 'country', 'city']:
            resolved = f"{extracted_topic} in {message}"
        else:
            resolved = f"{extracted_topic} {message}"
        
        logger.info(f"‚úÖ Resolved answer: '{message}' ‚Üí '{resolved}'")
        return resolved
    
    def _extract_current_message(
        self,
        resolved_full: str,
        original_message: str,
        message_index: int
    ) -> str:
        """
        Extract the resolved version of the current message from full text
        """
        # Split by sentences and get the last one(s) that match the original length
        sentences = resolved_full.split(". ")
        
        # Simple heuristic: take the last sentence(s) that roughly match original length
        if len(sentences) > 0:
            return sentences[-1].strip()
        
        return resolved_full
    
    def _find_replacements(
        self,
        original: str,
        resolved: str
    ) -> List[Dict[str, Any]]:
        """
        Find what was replaced in the resolution
        """
        import difflib
        
        replacements = []
        
        # Use difflib to find differences
        matcher = difflib.SequenceMatcher(None, original, resolved)
        
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'replace':
                replacements.append({
                    "original": original[i1:i2],
                    "resolved": resolved[j1:j2],
                    "confidence": 0.8,  # Placeholder confidence
                    "position": i1
                })
        
        return replacements
    
    def _check_needs_context(
        self,
        message: str,
        replacements: List[Dict[str, Any]],
        method: str
    ) -> bool:
        """
        Determine if this query needs conversation context to be understood.
        
        Returns True if:
        1. Coreferences were found and resolved (replacements exist)
        2. Query contains elliptical reference patterns (ordinals, follow-ups, etc.)
        3. Query is very short (< 5 words) - likely a follow-up
        
        Args:
            message: The original message
            replacements: List of coreference replacements made
            method: The coreference resolution method used
            
        Returns:
            bool: True if conversation context is needed
        """
        import re
        
        # 1. If coreferences were found, definitely needs context
        if replacements and len(replacements) > 0:
            logger.info(f"‚úÖ needsContext=True: Found {len(replacements)} coreference replacement(s)")
            return True
        
        # 2. Check for elliptical reference patterns
        elliptical_patterns = [
            r'\b(first|second|third|fourth|fifth|next|last|previous|another|other|more|else|too|also)\b',
            r'\b(what about|how about|tell me about)\b',
            r'\b(it|that|this|them|those|these)\b',
            r'^(why|how|when|where|who|which)\b',  # Questions starting with interrogatives
            r'\b(the .+)\b',  # Definite articles often refer to previous context
        ]
        
        message_lower = message.lower()
        for pattern in elliptical_patterns:
            if re.search(pattern, message_lower, re.IGNORECASE):
                logger.info(f"‚úÖ needsContext=True: Detected elliptical pattern '{pattern}' in message")
                return True
        
        # 3. Very short queries (< 5 words) are often follow-ups
        word_count = len(message.split())
        if word_count < 5:
            logger.info(f"‚úÖ needsContext=True: Short query ({word_count} words)")
            return True
        
        # 4. No indicators found - query is likely standalone
        logger.info(f"‚ùå needsContext=False: No context indicators found")
        return False
