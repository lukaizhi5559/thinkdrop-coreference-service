# Coreference Resolution MCP Service

A Python-based microservice for resolving pronouns and references in conversation messages using NLP.

## Overview

This service uses spaCy + neuralcoref to resolve coreferences like:
- **Pronouns:** "he", "she", "it", "they" â†’ actual person/entity names
- **References:** "the show", "the cartoon", "the movie" â†’ specific titles
- **Demonstratives:** "that", "this" â†’ specific entities

## Features

- âœ… **Neural coreference resolution** using neuralcoref
- âœ… **Rule-based fallback** for reliability
- âœ… **MCP protocol compliant** (works with existing services)
- âœ… **FastAPI** for high performance
- âœ… **Conversation context** aware
- âœ… **Graceful degradation** if NLP models fail

---

## Installation

### 1. Create Virtual Environment

```bash
cd mcp-services/coreference-service
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Download spaCy Model

```bash
python -m spacy download en_core_web_sm
```

### 4. Install neuralcoref

```bash
pip install neuralcoref
```

**Note:** If neuralcoref installation fails (it's not maintained for latest spaCy), the service will fall back to rule-based resolution.

### 5. Configure Environment

```bash
cp .env.example .env
# Edit .env and set your API key
```

---

## Running the Service

### Development Mode

```bash
python server.py
```

The service will start on `http://localhost:3005`

### Production Mode

```bash
uvicorn server:app --host 0.0.0.0 --port 3005 --workers 4
```

---

## API Endpoints

### Health Check

```bash
GET /health
```

**Response:**
```json
{
  "status": "ok",
  "service": "coreference",
  "version": "1.0.0"
}
```

### Resolve Coreferences

```bash
POST /resolve
```

**Request:**
```json
{
  "version": "mcp.v1",
  "service": "coreference",
  "action": "resolve",
  "requestId": "req_123",
  "payload": {
    "message": "who was the villain of the show",
    "conversationHistory": [
      {"role": "user", "content": "When did Transformers come out?"},
      {"role": "assistant", "content": "1984"}
    ],
    "options": {}
  }
}
```

**Response:**
```json
{
  "version": "mcp.v1",
  "service": "coreference",
  "action": "resolve",
  "requestId": "req_123",
  "status": "ok",
  "data": {
    "originalMessage": "who was the villain of the show",
    "resolvedMessage": "who was the villain of Transformers",
    "replacements": [
      {
        "original": "the show",
        "resolved": "Transformers",
        "confidence": 0.95,
        "position": 23
      }
    ],
    "method": "neuralcoref"
  }
}
```

---

## Integration with Main App

### 1. Add to MCP Configuration

Update your main app's `.env`:

```bash
# MCP Service Endpoints
MCP_COREFERENCE_ENDPOINT=http://localhost:3005
MCP_COREFERENCE_API_KEY=your-api-key-here
MCP_COREFERENCE_TIMEOUT=5000
```

### 2. Register Service in Database

The service will be auto-registered on first health check, or manually add:

```sql
INSERT INTO mcp_services (id, name, display_name, endpoint, api_key, enabled, trusted)
VALUES (
  'coreference',
  'coreference',
  'Coreference Resolution',
  'http://localhost:3005',
  'your-api-key-here',
  true,
  true
);
```

### 3. Add Actions

```sql
-- Add to service_actions or update in migration
-- Actions: resolve
```

### 4. Create Node in StateGraph

Create `/src/main/services/mcp/nodes/resolveReferences.cjs`:

```javascript
module.exports = async function resolveReferences(state) {
  const { mcpClient, message, conversationHistory } = state;
  
  try {
    const result = await mcpClient.callService('coreference', 'resolve', {
      message,
      conversationHistory: conversationHistory.slice(-5), // Last 5 messages
      options: {}
    });
    
    const resolvedMessage = result.data?.resolvedMessage || message;
    
    return {
      ...state,
      message: resolvedMessage, // Replace with resolved message
      originalMessage: message,
      coreferenceReplacements: result.data?.replacements || []
    };
  } catch (error) {
    console.warn('âš ï¸ Coreference resolution failed, using original message');
    return state; // Graceful fallback
  }
};
```

### 5. Update StateGraph

Add the node before `answer`:

```javascript
// In StateGraph.cjs
const resolveReferences = require('./nodes/resolveReferences.cjs');

// Add to graph
graph.addNode('resolveReferences', resolveReferences);

// Add edge: filterMemory â†’ resolveReferences â†’ answer
graph.addEdge('filterMemory', 'resolveReferences');
graph.addEdge('resolveReferences', 'answer');
```

---

## Testing

### Manual Test

```bash
curl -X POST http://localhost:3005/resolve \
  -H "Content-Type: application/json" \
  -H "X-API-Key: your-api-key-here" \
  -d '{
    "version": "mcp.v1",
    "service": "coreference",
    "action": "resolve",
    "requestId": "test_123",
    "payload": {
      "message": "who was the villain of the show",
      "conversationHistory": [
        {"role": "user", "content": "When did Transformers come out?"},
        {"role": "assistant", "content": "1984"}
      ]
    }
  }'
```

### Integration Test

1. Start all services (including coreference)
2. Ask: "When did Transformers come out?"
3. Ask: "who was the villain of the show"
4. Check logs to see resolved message: "who was the villain of Transformers"

---

## Resolution Methods

### 1. Neuralcoref (Best)

Uses neural network to understand context and resolve references accurately.

**Pros:**
- High accuracy
- Handles complex cases
- Context-aware

**Cons:**
- Requires neuralcoref installation
- Slower (~100-200ms)

### 2. Rule-Based (Fallback)

Simple pattern matching and proper noun extraction.

**Pros:**
- Fast (~10-20ms)
- No dependencies
- Reliable for simple cases

**Cons:**
- Less accurate
- Misses complex references

---

## Performance

- **Latency:** 50-200ms per request (depending on method)
- **Memory:** ~500MB (spaCy model loaded)
- **Throughput:** ~100 requests/second (single worker)

### Optimization Tips

1. **Use multiple workers:**
   ```bash
   uvicorn server:app --workers 4
   ```

2. **Cache model in memory** (already done)

3. **Limit conversation history** (only last 5 messages)

4. **Use rule-based for simple cases** (set `USE_NEURALCOREF=false`)

---

## Troubleshooting

### neuralcoref Installation Fails

**Solution:** Use rule-based fallback:
```bash
# In .env
USE_NEURALCOREF=false
```

### spaCy Model Not Found

**Solution:** Download the model:
```bash
python -m spacy download en_core_web_sm
```

### Service Crashes on Startup

**Check:**
1. Python version >= 3.8
2. All dependencies installed
3. Port 3005 not in use

---

## Development

### Project Structure

```
coreference-service/
â”œâ”€â”€ server.py                    # FastAPI app
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Environment template
â”œâ”€â”€ README.md                    # This file
â””â”€â”€ src/
    â”œâ”€â”€ coreference_resolver.py  # Core resolution logic
    â”œâ”€â”€ middleware/
    â”‚   â””â”€â”€ validation.py        # MCP validation
    â””â”€â”€ routes/
        â””â”€â”€ resolve.py           # API routes
```

### Adding New Resolution Methods

Edit `src/coreference_resolver.py` and add your method to the `resolve()` function.

---

## Deployment

### Docker (Recommended)

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
RUN python -m spacy download en_core_web_sm

COPY . .

EXPOSE 3005

CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "3005"]
```

Build and run:
```bash
docker build -t coreference-service .
docker run -p 3005:3005 --env-file .env coreference-service
```

---

## License

Same as main project

---

## Status

âœ… **Ready for testing**  
ğŸ”„ **Integration pending** (need to add to StateGraph)  
ğŸ“ **Documentation complete**
